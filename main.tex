\documentclass{scrartcl}

\usepackage{amsmath}
\usepackage{amssymb}

%Numbers, expectation
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\V}{\mathbb{V}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\G}{\mathcal{G}}

%%epigraph,domain
\newcommand{\dom}{\textup{dom}}
\newcommand{\epi}{\textup{epi}}
\newcommand{\interior}{\textup{int}}
\newcommand{\intdom}{\interior \, \dom}

%%other
%\renewcommand{\complement}{\mathsf{c}}
%\renewcommand{\implies}{\Rightarrow}
\newcommand{\sle}{<}
\newcommand{\sge}{>}
\newcommand{\sub}{\subseteq}
% converges to
\newcommand{\convto}[1]{\overset{#1 \to \infty}{\longrightarrow}}
% convex hull
\newcommand{\conv}[1]{\textup{conv $\left(#1 \right)$}}
% set-valued map
\newcommand{\svarrow}{\rightrightarrows}

%% mathoperator
%argmax
\DeclareMathOperator*{\argmax}{arg\,max}
%argmin
\DeclareMathOperator*{\argmin}{arg\,min}
%signum
\DeclareMathOperator{\sgn}{sgn}
%\DeclareMathOperator{\prox}{\proximal}
%\newcommand[2]{\proximal}{\prox_{#1}\left(#2 \right)}
\newcommand{\prox}[3][]{\operatorname{prox}^{#1}_{#2}\left(#3 \right)}
%\newcommand{\interior}{\textup{int}}

%%
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}[subsection]{Theorem}
\newtheorem{corollary}[subsection]{Corollary}
\newtheorem{defin}[subsection]{Definition}
\newtheorem{lemma}[subsection]{Lemma}
\newtheorem{prop}[subsection]{Proposition}
\newtheorem{problem}[subsection]{Problem}
\newtheorem{algo}[subsection]{Algorithm}
\newtheorem{example}[subsection]{Example}
\newtheorem{assumption}{Assumption}
\renewcommand*{\theassumption}{\Alph{assumption}}

\theoremstyle{remark}
%\newtheorem*{rem}{Remark}
\newtheorem{remark}[subsection]{Remark}

%%big times
\usepackage{mathabx}

% for tikz pictures
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\newlength{\figureheight}
\newlength{\figurewidth}


%qed symbol
\newcommand*{\QED}{\hfill\ensuremath{\square}}%

%% sort citations by increasing number
\usepackage[sort,nocompress]{cite}

%% extended reals
\usepackage{amsfonts}
\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

%% change items in enumerate quickly
\usepackage{enumerate}

% \left and \right spanning over multiple lines
\usepackage{breqn}

% d for integral
\newcommand*\diff{\mathop{}\!\mathrm{d}}

% norm with 3 lines
\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1
		\right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}

% regular norm, scalar product
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\vertii}{\Vert}{\rVert}
\DeclarePairedDelimiter{\inner}{\langle}{\rangle}


% bold math symbols
\usepackage{bm}
\usepackage{bbm}

% zeros
\DeclareMathOperator*{\zer}{\textup{zer}}

% Filtrations
\newcommand{\F}{\mathcal{F}}
\newcommand{\Filt}{\mathscr{F}}
\usepackage{mathrsfs}
\newcommand{\Borel}{\mathcal{B}}

% identity
\newcommand{\Id}{\textup{Id}}

% show labels in pdf
\usepackage{refcheck}


\usepackage{mathtools}
\mathtoolsset{showonlyrefs}


\renewcommand{\P}{\textup{P}}
\newcommand{\clconv}{\overline{\textup{conv}}\,}
\newcommand{\ran}{\textup{ran}\,}
\newcommand{\clran}{\overline{\textup{ran}}\,}
\newcommand{\Fix}{\textup{Fix}\,}
\newcommand{\ov}{\textcircled{v}}
\newcommand{\gr}{\textup{gra}\,}



\begin{document}
\textbf{Problem 1} Consider $D \sub C \sub \H$, where $C$ is closed convex and we assume that $\P_C(0) \in D$.\\
(i) Show that $D$ has the minimum norm property and that $\P_C(0) = \P_D(0)$. \\
From the definition of the Projection we deduce 
\begin{equation}
  \lVert \P_C(0) \rVert \le \lVert x \rVert, \quad \forall x \in C
\end{equation}
thus, as $\clconv(D) \sub C$,
\begin{equation}
  \label{eq:clconv}
  \lVert \P_C(0) \rVert \le \lVert x \rVert, \quad \forall x \in \clconv(D)
\end{equation}
and therefore
\begin{equation}
  \label{eq:projond}
  \lVert \P_C(0) \rVert \le \lVert x \rVert, \quad \forall x \in D.
\end{equation}
By assumption $\P_C(0) \in D$ and therefore $\P_C(0) \in \clconv(D)$.
Combining this with~\eqref{eq:clconv} gives that $\P_C(0) = \P_{\clconv(D)}(0)$ and proofs the minimum norm property of $D$.
Similarly, we deduce from~\eqref{eq:projond} that $\P_C(0) \in D$ is already the projection of $0$ onto $D$, i.e. $\P_C(0) = \P_D(0)$.

(ii)
From the projection theorem we deduce that
\begin{equation}
  \label{eq:lala}
  \left\langle x_{k} - \P_C(0), 0 - \P_C(0) \right\rangle \le 0
\end{equation}
and therefore
\begin{equation}
  \left\langle x_{k}, \P_C(0) \right\rangle \ge \lVert \P_C(0) \rVert^2.
\end{equation}
Furthermore, by Cauchy-Schwarz also have that
\begin{equation}
  \lVert \P_C(0) \rVert \lVert x_{k} \rVert \ge \left\langle x_{k}, \P_C(0) \right\rangle \ge \lVert \P_C(0) \rVert^2.
\end{equation}
% Let us now consider an arbitrary weakly convergent subsequence, which must exist as $(x_{k})$ is bounded, and call its limit 
% Let $\bar{x}$ be any weak clusterpoint of $(x_{n})$, then $\left\langle \bar{x}, \P_{C}(0) \right\rangle = \lVert \P_{C}(0) \rVert^2$ and $\lVert \bar{x} \rVert = \lVert \P_{C}(0) \rVert$. Thus, $\bar{x} = \P_{C}(0)$.
% Assume now, on the contrary that there exists a subsequence of $(x_{n_k})$ which does not converge to $\P_C(0)$.
Thus $\left\langle x_{k}, \P_C(0) \right\rangle$ converges to $\lVert \P_C(0) \rVert^{2}$.
Now consider
\begin{equation}
  \begin{aligned}
    \lVert x_{k} - \P_{C}(0) \rVert &= \left\langle x_{k} - \P_{C}(0), x_{k} - \P_{C}(0) \right\rangle \\
    &= \left\langle x_{k} - \P_{C}(0), 0 - \P_{C}(0) \right\rangle + \left\langle x_{k} - \P_{C}(0), x_{k} \right\rangle  \\
    &\le \lVert x_{n} \rVert^2 - \left\langle \P_{C}(0), x_{k} \right\rangle 
  \end{aligned}
\end{equation}
which proves the strong convergence.

\textbf{Remark (see Problem 2 and 3)} 
Note that the projection onto a closed convex set is a firmly nonexpansive map and identity minus firmly nonexpansive map is again firmly nonexpansive.
Thus, if we look for a projection $P$ in place of $\Id - T$ we can ensure that $T = \Id - P = \Id - (\Id - T)$ is in fact nonexpansive.

\textbf{Problem 2} Construct an example where $\clran(\Id - T)$ is not convex, for a nonexpansive operator $T:C \to C$.\\
Consider $C= \{(x,y) \in \R^2 : y \ge 0 \}$ the uppe half space and let $\P := \Id - T$ be the projection onto the unit ball with center $(0,-1)$.
First it is easy to see that that the image of $C$ under the projection is nonconvex as is given by the upper half of surface of the ball (not including the points $(1,-1)$ and $(-1,-1)$).
Now we need to check that $\ran(T) = \ran (\Id - \P)$ is contained in the upper half space (since we require that $T$ maps back to its domain).
This is however clear as the domain is above the $x$-axis and all images of $\P$ are below it. Thus, all vecors given by $x - \P x$ point up (and are therefore contained in $C$). \\

\textbf{Problem 3} Construct an example where $\clran(\Id - T)$ does not posses the minimum norm propert for a nonexpansive operator $T:C \to X$. \\
Let $\Id - T$ be the projection onto the unit ball, in $\R^2$ with domain $\{(x, 1) \in \R^2 : -1 \le x\le 1\}$.
The range of $\Id - T$ is then clearly a subset of the upper half of the unit circle and does not have the minimum norm property. \\


\textbf{Problem 4} Pazy's Trichotomy.\\
i)$T=\Id$.\\
Clearly $0 \in \clran(\Id - T) = \{0\}$ and $(T^{n}x) = x$ for all $n$ is therefore bounded.

ii) Let $T: \R \to \R$  be defined by
\begin{equation}
  T(x) := 
  \begin{cases}
    x+1, & \text{if} x\le1; \\
    x+\frac{1}{x}, & \text{if} x > 1.
  \end{cases}
\end{equation}
Clearly, $\ran(\Id - T) = [-1, 0)$.
This means that case ii) of Pazy's Trichotomy is occurring. 
Now we want to check that $\limsup_n T^n x = +\infty$ for some/all $x$. \\
Clearly, for any $x$ we have that $T^n x$ is greater than $1$ after finitely many steps, so we only consider this case.
For $x>1$ we can see that $T$ is monotone. Let $x>y > 1$, then
\begin{equation}
  Tx \ge Ty \Leftrightarrow x + \frac{1}{x} \ge y + \frac{1}{y} \Leftrightarrow x-y \ge \frac{1}{y} - \frac{1}{x} = \frac{x-y}{xy}
\end{equation}
where the right hand side $x-y \ge \frac{x-y}{xy}$ is true as $x,y > 1$.



\textbf{Problem 5} The Edelstein operator.
It is given by 
\begin{equation}
  T\left({(x_{k})}_{k \in \N}\right) := {\left( 1 + (x_{k} - 1)\exp \left(\frac{2 \pi i}{k!}\right)\right)}_{k \in \N}.
\end{equation}

Check that ${(T^{n}0)}_{k} = 1 - \exp \left(\frac{2 \pi i n}{k!}\right)$.\\

For $n=1$, this follows immediately from the definition.
Assume it is true for $n-1$. Then,
\begin{equation}
  \begin{aligned}
    {(T^{n}0)} = {(T \circ T^{n-1}0)} = T \left( {\left(1 - \exp \left(\frac{2 \pi i n}{k!}\right)\right)}_{k \in \N} \right) \\
    = {\left( 1 - \exp\left(\frac{2 \pi i (n-1)}{k!}\right)\exp \left(\frac{2 \pi i}{k!}\right)\right)}_{k \in \N} \\
    = {\left( 1 - \exp \left(\frac{2 \pi i n }{k!}\right)\right)}_{k \in \N}.
  \end{aligned}
\end{equation}
Next, for $x \in \ell^2$ we have that $\lVert T^{k}x - T^{k+1}x \rVert = \lVert Tx - x \rVert$ since $T$ is an isometry.
Thus, $\lim_{k} \lVert T^{k}x - T^{k+1}x \rVert = \lVert Tx - x \rVert$, which is strictly greater zero as $T$ has no fixed points.\\


\textbf{Problem 6} Let $T: \H \to \H$ be linear, and nonexpansive. Show that $\Fix T = \Fix T^*$.
\begin{equation}
  x \in \Fix T \Leftrightarrow Tx=x \implies \left\langle Tx, x \right\rangle = \lVert x \rVert^2 = \left\langle T^*x, x \right\rangle.
\end{equation}
This shows that 
\begin{equation}
  \left\langle T^*\frac{x}{\lVert x \rVert}, \frac{x}{\lVert x \rVert} \right\rangle = 1
\end{equation}
which means that the angle between the two vectors is zero and since $\lVert T \frac{x}{\lVert x \rVert} \rVert \le 1$ it must already be one and the two vectors have to be equal. Thus $x$ is also a fixed point of $T^*$.\\

\textbf{Problem 7} Let ${(\alpha_{k})}_{k \in \N}$ be a sequence of real numbers converging to $\lambda \in \R$. We show that
\begin{equation}
  \lim_{k \to \infty} \frac{1}{k} \sum_{i=1}^{k} \alpha_{k} = \lambda.
\end{equation}
Let $\epsilon>0$ be arbitrary and choose $K$ so large that $\lvert \alpha_{i} - \lambda \rvert < \epsilon$ for all $i\ge K$. Then,
\begin{equation}
  \left\lvert \frac{1}{k} \sum_{i=1}^{k} \alpha_{k} - \lambda \right\rvert = \frac{1}{k} \left\lvert \left(\sum_{i=1}^{K} \alpha_{i} - \lambda\right) + \left( \sum_{i=K}^{k} \alpha_{i} - \lambda \right) \right\rvert \le \frac{const.}{k} + \epsilon.
\end{equation}
The only thing left to do is to choose $k$ large enough such that $\frac{const.}{k} \le \epsilon$.\\

\textbf{Problem 8} Show that $A^{-\ov}$ is good notation.
\begin{equation}
  \label{eq:ov}
  {(A^{\ov})}^{-1} = {(-\Id)}^{-1} \circ A^{-1} \circ {(-\Id)}^{-1} = (-\Id) \circ A^{-1} \circ (-\Id) = {(A^{-1})}^{\ov}
\end{equation}\\

\textbf{Problem 9} Show that ${(A, B)}^{**} := {({(A, B)}^*)}^* = {(A, B)}$.
Clearly ${(A^{-1})}^{-1} = A$. So the only thing to check is that
\begin{equation}
  {\left(B^{-\ov}\right)}^{-\ov} = B.
\end{equation}
By~\eqref{eq:ov}
\begin{equation}
  \begin{aligned}
    {\left(B^{-\ov}\right)}^{-\ov} &= {\left({\left({\left(B^{\ov}\right)}^{-1}\right)}^{-1}\right)}^{\ov} = {\left(B^{\ov}\right)}^{\ov}\\
    &= {(-\Id)} \circ B^{\ov} \circ {(-\Id)} \\
    &= {(-\Id)} \circ {(-\Id)} \circ B \circ {(-\Id)} \circ {(-\Id)} \\
    &= B.
  \end{aligned}
\end{equation}\\

\textbf{Problem 10 (Attouch-Thera duality)} For maximally monotone $A,B$ we define $K_{z} := (Az) \cap (-Bz)$ and $Z_{k} := A^{-1}k \cap B^{-1}(-k)$. Clearly, $Z_{k}$ is a closed, convex set for any $k$ as the image of of maximally monotone operator is always closed and convex and so is their intersection.
Furthermore we have that
\begin{equation}
  \begin{aligned}
    & k \in K_{z} \Leftrightarrow k \in (Az) \cap (-Bz) \\
    \Leftrightarrow& k \in Az \wedge k \in -Bz \\
    \Leftrightarrow& z \in A^{-1}(k) \wedge z \in B^{-1}(-k) \\
    \Leftrightarrow& z \in (A^{-1}k) \cap (B^{-1}(-k)) \\
    \Leftrightarrow& z \in Z_{k}.
  \end{aligned}
\end{equation}\\
Also, all the solution to $0 \in Ax + Bx$ which we call $Z:= {(A + B)}^{-1}(0)$ are given by $\bigcup_{k \in X} Z_{k}$. To see this, let $z \in Z$ be a solution. Then $K_{z}$ is not empty, i.e. it contains an element $k$. 
Thus, $z \in Z_{k}$ and therefore $z \in \bigcup_{k \in X} Z_{k}$.\\
Conversely, let $z$ be an element of $\bigcup_{k \in X} Z_{k}$. Then, there exists a $k$, such that $z \in Z_{k}$. Thus, $k \in K_{z}$ which means that $K_{z}$ is in particular not empty and $z$ is therefore a solution.

\textbf{Problem 11 (Passty's convexity result)} Show that
\begin{equation}
  (\gr A) \cap ((x,0) - \gr (-B)) = \{(y,w) \in \gr A \,| \,(x-y,w) \in \gr B\}.
\end{equation}
This follows directly from the definition of intersection
\begin{equation}
  \begin{aligned}
    (\gr A) \cap ((x,0) - \gr (-B)) &= \{(y,w) \,| \,(y,w) \in \gr A \wedge (x,0) - (y,-w) \in \gr B\} \\
    &= \{(y,w) \,| \,(y,w) \in \gr A \wedge (x-y,w) \in \gr B\} \\
    &= \{(y,w) \in \gr A \,| \,(x-y,w) \in \gr B\}.
  \end{aligned}
\end{equation}\\

\textbf{Problem 12 (parallel sum)} Show that
\begin{equation}
  (A \square B)z = \bigcup_{y \in X} (Ay) \cap (B(x-y)).
\end{equation}
We start of by proofing that
\begin{equation}
  \bigcup_{y \in X} (Ay) \cap (B(x-y)) \sub (A \square B)z.
\end{equation}
Let $z \in \bigcup_{y \in X} (Ay) \cap (B(x-y))$.
Then there exists a $y'$ such that
\begin{equation}
  z \in (Ay') \cap (B(x-y')).
\end{equation}
Thus 
\begin{equation}
  z \in A y' \quad \wedge \quad z \in B(x-y').
\end{equation}
Therefore, by inverting the operators
\begin{equation}
  y' \in A^{-1} z \quad \wedge \quad x - y' \in B^{-1}z.
\end{equation}
Summing up the above inclusions gives
\begin{equation}
  x \in (A^{-1} + B^{-1})z.
\end{equation}
Inverting again gives the desired inclusion. The opposite inclusion follows analogously.\\

\textbf{Problem 13 (paramonotone operators)} Show that $A+\lambda B$ is paramonotone for paramonotone operators $A,B$.
Let $(x,x^*) \in  \gr (A+\lambda B)$ and $(y,y^*) \in \gr (A + \lambda B)$ where $x^* = x_A^* + \lambda x_B^*$ for $x_A^* \in Ax$ and $x_B^* \in Bx$ - analogously for $y^*$.
Assume now that
\begin{equation}
  \left\langle x-y, x_A^* + \lambda x_B^* - y_A^* - \lambda y_B^* \right\rangle = 0.
\end{equation}
Then, by linearity of the inner product we get
\begin{equation}
  \left\langle x-y, x_A^*  - y_A^*  \right\rangle + \lambda \left\langle x-y, x_B^* - y_B^* \right\rangle = 0.
\end{equation}
Since $A$ and $B$ are monotone both of these summands are nonnegative so they both must be zero.
Due to the paramonotonicity we get that
\begin{equation}
  \{(x,y_A^*), (y, x_A^*) \} \sub \gr A
\end{equation}
and
\begin{equation}
  \{(x,y_B^*), (y, x_B^*) \} \sub \lambda \, \gr B.
\end{equation}
Thus,
\begin{equation}
  \{(x, y_A^* + y_B^*), (y, x_A^* + x_B^*)\} \sub \gr (A + \lambda B)
\end{equation}
which proofs the paramonotonicity of $A+\lambda B$.\\


\textbf{Problem 14 (orthogonal sum)} Let $C, D$ be nonempty closed convex sets.
i) $C+D$ is convex.
\begin{equation}
  C+D = \{c+d : c \in C, d \in D\}
\end{equation}
Let $x,y \in C+D$, then for any $\alpha \in (0,1)$
\begin{equation}
  \begin{aligned}
    \alpha x+ (1 -\alpha)y &= \alpha(c_x + d_x) + (1-\alpha)(c_y + c_d)\\
    &= \alpha c_x + (1-\alpha) c_y + \alpha c_y (1-\alpha) d_y
  \end{aligned}
\end{equation}
which is again an element of $C+D$ due to the convexity of $C$ and $D$.
ii) If $C \perp D$, then $C+D$ is closed. \\
Let $(x_{n})_{n \in \N}$ be a convergent sequence in $C+D$ with limit $x$. Due to the orthogonality of $C$ and $D$ we can decompose $x_{n} = c_{n} + d_{n}$ in a unique way, for $c_{n} \in C$ and $d_{n} \in D$. In particular, $c_{n} = \P_{C}x_{n}$ and analogously $d_{n} = \P_{D}x_{n}$. Due to the nonexpansiveness of the projection 
\begin{equation}
  \lVert x_{n} - x \rVert \ge \lVert \P_{C}x_{n} - \P_{C}x \rVert
\end{equation}$c_{n}$ and $d_{n}$ must also converge.

iii) Example where $C+D$ is not closed.\\
Consider $C=\{(x,y) : e^{-x} \le y\}$ and $D = \{(-x,0) : x \ge 0\}$. Then, $(0, y) \in C+D$ for all $y>0$ as we can decompose it through
\begin{equation}
  (0, y) = (-x, 0) + (x, y)
\end{equation}
for an arbitrary $x$ such that $e^{-x} \le y$. However, the origin is clearly not contained, making the sum not closed.\\

\textbf{Problem 15 (projector onto primal solutions)} Let $A,B$ be paramonotone and $Z-Z \perp K$. Show that $J_A \circ \P_{Z+K} = \P_Z$.\\
Let $z_0 \in Z$. Then $Z-z_0 \perp K$. Thus, $Z+K -z_0$ is closed and convex.
Therefore,
\begin{equation}
  \begin{aligned}
    \P_{Z+K}(x) &= \P_{z_0+ Z+K}(x) = z_0 + \P_{Z+K-z_0}(x-z_0) \\
    &= z_0 + \P_{Z-z_0}(x-z_0) + \P_{K}(x-z_0) = \P_Z(x) + \P_K(x-z_0).
  \end{aligned}
\end{equation}
Set now $z := \P_Z(x)$.
Then,
\begin{equation}
  \P_{Z+K}(x) - z = \P_{Z+K}(x) - \P_Z(x) = P_K(x) \in K = K_z \sub Az.
\end{equation}
Hence,
\begin{equation}
  z = J_A \circ \P_{Z+K}(x)
\end{equation}
i.e.
\begin{equation}
  \P_Z(x) = J_A \circ \P_{Z+K}(x).
\end{equation}


\textbf{Problem 16 (reflected resolvent calculus)} Check the following calculus. \\
i) $R_{C^{-1}} = - R_{C}$ \\
Let $y = R_{C^{-1}}(x)$, then
\begin{equation}
  \begin{aligned}
    &y = 2{\left(\Id + C^{-1}\right)}^{-1}(x) - x \\
    \Leftrightarrow& \frac{y+x}{2} = {\left(\Id + C^{-1}\right)}^{-1}(x) \\
    \Leftrightarrow& \frac{y+x}{2} + C^{-1}\left(\frac{y+x}{2}\right) \ni x \\
    \Leftrightarrow& C^{-1}\left(\frac{y+x}{2}\right) \ni \frac{x-y}{2} \\
    \Leftrightarrow& \frac{y+x}{2} \in  C \left( \frac{x-y}{2} \right) \\
    \Leftrightarrow& x \in \frac{x-y}{2} + C \left( \frac{x-y}{2} \right) \\
    \Leftrightarrow& {(\Id + C)}^{-1}(x) = \frac{x-y}{2} \\
    \Leftrightarrow& x-2{(\Id + C)}^{-1}(x) = y \\
    \Leftrightarrow& -R_{C}(x) = y.
  \end{aligned}
\end{equation}

ii) $J_{C^{\ov}} = {(J_{C})}^{\ov}$ \\
Let $y= J_{C^{\ov}}$, then
\begin{equation}
  \begin{aligned}
    &y= {(\Id + (-\Id) \circ C \circ (-\Id))}^{-1}(x) \\
    \Leftrightarrow& x \in y - C(-y) \\
    \Leftrightarrow& x \in - ( -y + C(-y)) \\
    \Leftrightarrow& x \in - {(\Id + C)}(-y) \\
    \Leftrightarrow& -{(\Id + C)}^{-1}(-x) = y \\
    \Leftrightarrow& {(J_{C})}^{\ov}(x) = y.
  \end{aligned}
\end{equation}

iii) Show that $R_{C^{-\ov}} = \Id - 2{(J_{C})}^{\ov}$.\\
Utilizing i) and ii) we deduce that
\begin{equation}
  R_{C^{-\ov}} = R_{\left(C^{\ov}\right)^{-1}} \overset{i)}{=} - R_{C^{\ov}} = \Id - 2 J_{C^{\ov}} \overset{ii)}{=} \Id - 2 {(J_{C})}^{\ov}.
\end{equation}



\textbf{Problem 17 (backward-backward operator)} Give an example where backward-backward is not self-dual.
Let $B$ be the normal cone to $\R_{+}$ and $A$ the identity on $\R$.
Then, 
\begin{equation}
  J_{B} \circ J_{A} (-1) = J_{B} (-1) = 0
\end{equation}
and
\begin{equation}
  J_{B^{-\ov}} \circ J_{A^{-1}} (-1) = J_{B^{-\ov}} (-1) = (-\Id) \circ B^{-1} (1) = (-\Id) (1) = -1.
\end{equation}


\textbf{Problem 18} Let $A,B$ be paramonotone and $k \in K$. Assume $J_A(z+k) = \P_Z(z=k)$ for all $z \in Z$.
Show that $k \in (Z -Z)^{\perp}$.\\
As a first small interlude, we will proof that 
\begin{equation}
  J_A(z+k) = z.
\end{equation}
We probably proofed this at some point but I couldn't find it (or it can very easily deduced from our work about Douglas-Rachford).
Either way, since $A$ and $B$ are paramonotone $K=K_z$ for all $z \in Z$.
This means that every $k \in K$ fullfills 
\begin{equation}
  k \in Az \cap (-Bz)
\end{equation}
for all $z \in Z$, and in particular 
\begin{equation}
  k \in Az.
\end{equation}
Thus $k+z \in (\Id + A)(z)$ and 
\begin{equation}
  J_A(z+k) = z.
\end{equation}\\


Now we can use the assumption of this problem that $J_A(z+k) = \P_Z(z+k)$ for all $z \in Z$ to deduce that
\begin{equation}
  z = P_Z(z+k) \quad, \forall z \in Z.
\end{equation}
From this we deduce via the projection theorem that
\begin{equation}
  \left\langle z-z', z+k - z \right\rangle\le 0, \quad \forall z,z' \in Z.
\end{equation}
Thus
\begin{equation}
  \left\langle z-z', k \right\rangle\le 0, \quad \forall z,z' \in Z.
\end{equation}
And by reversing the roles of $z$ and $z'$ also
\begin{equation}
  \left\langle z'-z, k \right\rangle\le 0, \quad \forall z,z' \in Z.
\end{equation}
Therefore
\begin{equation}
  k \perp (Z-Z).
\end{equation}

\textbf{Problem 19} Let $U$ be a closed linear subspace of $X$ and $b \in U^{\perp}\\ \{0\}$, $A:= N_U$ and $B=\Id + N_{-b+U}$. \\
From the paper we already know that
\begin{equation}
  J_B = -b +\frac{1}{2}\P_U
\end{equation}
and the Douglas-Rachford operator
\begin{equation}
  T = T_{(A,B)} = \P_{U^{\perp}} + J_B R_U.
\end{equation}
By induction we check that
\begin{equation}
  T^n = P_{U^{\perp}} + \frac{1}{2^n} \P_U - n b.
\end{equation}
In order to see this we consider
\begin{equation}
  T^{n+1} = T \circ T^{n} = (\P_{U^{\perp}} + (-b +\frac{1}{2}\P_U) R_U)\circ\left(P_{U^{\perp}} + {\left(\frac{1}{2}\right)}^{n} \P_U - n b\right).
\end{equation}
Expanding the above expressions gives
\begin{equation}
  \begin{aligned}
    &\left(\P_{U^{\perp}} + (-b +\frac{1}{2}\P_U) R_U\right)\circ\left(P_{U^{\perp}} + \frac{1}{2^n} \P_U - n b\right) = \\
    &= \P_{U^{\perp}} + 0 - nb + \left((-b +\frac{1}{2}\P_U) R_U\right) \circ \left(P_{U^{\perp}} + \frac{1}{2^n} \P_U - n b\right)\\
    &= \P_{U^{\perp}} - nb + - b + \frac{1}{2}\P_U R_U P_{U^{\perp}} + \frac{1}{2}\P_U R_U \frac{1}{2^n} \P_U  - \frac{1}{2}\P_U R_U(n b) \\
    &= \P_{U^{\perp}} - (n+1)b + 0 + \frac{1}{2}\P_U \frac{1}{2^n} \P_U - \frac{n}{2}\P_U ( b) \\
    &= \P_{U^{\perp}} - (n+1)b  + \frac{1}{2^{n+1}} P_U,
  \end{aligned}
\end{equation}
where used the fact that $R_U P_U = P_U R_U = P_U$ multiple times.
This finishes the proof.\\


\textbf{Coding Problem} Suppose $U = \R^2_+$ and $V$ a line in $\R^2$, not necessarely through the origin.
We set $A= N_U$ and $B=N_V$.
Clearly $J_A$ is then given by the the projection onto the nonnegative orthant which simply set negative coordinates of a point to zero.
Similarly $J_B$ is a projection onto the line which can be realized by subtracting a reference point, projection onto the corresponding $1$-dimensional subspace and then readding the reference point, where the the projection of a point $x$ onto the subspace genereated by a vector $v$ is given by the formula
\begin{equation}
  \frac{\left\langle x,v \right\rangle }{\lVert v \rVert^2} v.
\end{equation}

\begin{figure}%
  \includegraphics[scale=0.7]{./intersec_int.png}
  \centering
  \caption{The line has a nonempty intersection with the interior of the nonnegative orthant. Douglas-Rachford converges in finitely many steps to a solution and does so in much less iterations than the alternating projection method.}
\end{figure}

\begin{figure}%
  \includegraphics[scale=0.7]{./no_solution.png}
  \centering
  \caption{The intersection between line and nonnegative orthant is empty. This means that the problem is infeasible. The iterates thus converge to $+\infty$. The shadow sequence however converges to a normal solution. So do the iterates of the alternating projection method after slightly more iterations.} 
\end{figure}
\end{document}
